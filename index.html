<h1>DataSci Story 1</h1>

<h2>Introduction</h2>
<p>
	We've embarked on a mission to take a critical look at the way the United States treats those who cross its borders (whether that be refugees, undocumented immigrants, or legal permanent residents), and how those people react and live in the United States. We will be comparing data that we find representative of the government and society's treatment of these people to the statistics and stories that best represent their lives in the United States.
</p>

<h2>On the Treatment of Immigrants by the United States</h2>

<h3>Congress</h3>
<p>	
	We decided to explore the years of legislation in Congress 
</p>  

<h3>Department of Homeland Security</h3>

<p>
	We've gathered data from the Department of Homeland Security to investigate the time and money the United States spends on allowing or denying people access into the country. The DHS has data tables for a range of subjects, a few of which have data reaching back to 1820. For our project however, we will most likely be exploring no more than twenty or thirty years into the past, where the majority of their data sets are complete. We chose a 
</p>

<h3>Society</h3>
<p>
	During this part of the project, we try to analyze what is the society’s general impression of immigration. We choose to explore some news press like New York Times as the first step because we potentially want to compare the difference between people’s impression of immigration before and nowadays. Newspaper media is one of the very few things lasts all the time until nowadays. It is more durable than internet social media which only exists in the most recent decade. I went to the NYT API website http://developer.nytimes.com/docs and register for their article search api key which seems to be the most relevant api function call. I register around 10 api keys and download article infos from 1850 - 2006 about the keyword immigration. The article is selected if the keyword shows up on the headline or the body of the article. The return is a json file and I have to parse through to extract the info and gather together to store them in a csv file. After finally getting all the infos, I write a script to allow users to give a arbitrary bin number and then the program will automatically divide all the data across the years into those bins and extract the most important keywords and their frequencies based on each bin unit AND store them afterwards. I used sklearn’s TF IDF buit-in functions to handle the extraction.  As the next step, I am going to graph those words in a word cloud visualization based on different years. The sample graph could be like this http://vallandingham.me/bubble_cloud/ 
</p>

<h2>On the Situation of Immigrants in the United States:



